"""sai_pkg004 - Venomoussaversai init file

Auto-generated by GPT-5 (Venomoussaversai mode).
Package: sai_pkg004
Creator: Ananthu Sajeev
Purpose: Placeholder package init for Venomoussaversai project.
Generated: 2025-08-27
"""

# Package metadata
__version__ = "0.1.0"
__author__ = "Ananthu Sajeev"
__package_role__ = "sai_component"

# Example of package-level state that might be used by Venomoussaversai
_state = {
    "synced_with": "Venomoussaversai",
    "created_at": "2025-08-27",
    "notes": "Auto-generated init for package sai_pkg004"
}

def info():
    """Return a short info dict about this package."""
    return {
        "package": "sai_pkg004",
        "version": __version__,
        "author": __author__,
        "role": __package_role__,
        "notes": _state["notes"]
    }

# Hook for Venomoussaversai discovery
try:
    from importlib import metadata as _meta
    __dist_name__ = _meta.metadata(__package__) if __package__ else None
except Exception:
    __dist_name__ = None

# Minimal safety: do not run heavy initialization on import.
__initialized__ = False

def initialize():
    """Lightweight initialization hook for runtime -- safe to call repeatedly."""
    global __initialized__
    if __initialized__:
        return False
    # Place lightweight setup here (no blocking / heavy IO).
    __initialized__ = True
    return True
# tiny_gpt.py — minimal educational Transformer (toy, not production)
import math, torch, torch.nn as nn, torch.nn.functional as F

class MultiHead(nn.Module):
    def __init__(self, d_model, n_head):
        super().__init__()
        self.d_model, self.n_head = d_model, n_head
        self.d_k = d_model // n_head
        self.qkv = nn.Linear(d_model, 3*d_model, bias=False)
        self.o   = nn.Linear(d_model, d_model, bias=False)

    def forward(self, x):
        B, T, C = x.shape
        q, k, v = self.qkv(x).chunk(3, dim=-1)
        def split(t): return t.view(B, T, self.n_head, self.d_k).transpose(1,2)   # (B,h,T,d)
        q, k, v = map(split, (q,k,v))
        att = (q @ k.transpose(-2,-1)) / math.sqrt(self.d_k)
        mask = torch.triu(torch.ones(T,T, device=x.device), 1).bool()
        att = att.masked_fill(mask, float('-inf'))
        att = F.softmax(att, dim=-1)
        y = att @ v                                  # (B,h,T,d)
        y = y.transpose(1,2).contiguous().view(B,T,C)
        return self.o(y)

class Block(nn.Module):
    def __init__(self, d_model, n_head, d_mlp):
        super().__init__()
        self.ln1 = nn.LayerNorm(d_model)
        self.att = MultiHead(d_model, n_head)
        self.ln2 = nn.LayerNorm(d_model)
        self.ffn = nn.Sequential(
            nn.Linear(d_model, d_mlp), nn.GELU(), nn.Linear(d_mlp, d_model)
        )
    def forward(self, x):
        x = x + self.att(self.ln1(x))
        x = x + self.ffn(self.ln2(x))
        return x

class TinyGPT(nn.Module):
    def __init__(self, vocab_size, d_model=256, n_head=4, n_layer=4, d_mlp=1024, max_len=256):
        super().__init__()
        self.tok = nn.Embedding(vocab_size, d_model)
        self.pos = nn.Embedding(max_len, d_model)
        self.blocks = nn.ModuleList([Block(d_model, n_head, d_mlp) for _ in range(n_layer)])
        self.ln = nn.LayerNorm(d_model)
        self.head = nn.Linear(d_model, vocab_size, bias=False)
        self.max_len = max_len
    def forward(self, idx):
        B, T = idx.shape
        pos = torch.arange(T, device=idx.device)
        x = self.tok(idx) + self.pos(pos)[None, :, :]
        for blk in self.blocks: x = blk(x)
        x = self.ln(x)
        return self.head(x)

# quick demo (random “training” step on dummy data)
if __name__ == "__main__":
    torch.manual_seed(0)
    vocab, seq = 5000, 64
    model = TinyGPT(vocab, max_len=seq)
    opt = torch.optim.AdamW(model.parameters(), lr=3e-4)
    x = torch.randint(0, vocab, (8, seq))
    logits = model(x[:, :-1])
    loss = F.cross_entropy(logits.reshape(-1, vocab), x[:, 1:].reshape(-1))
    loss.backward(); opt.step()
    print("loss:", float(loss))